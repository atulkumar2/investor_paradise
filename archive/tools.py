from datetime import datetime, timedelta, date
from typing import Optional, cast, Annotated
import os
import pandas as pd
from types import SimpleNamespace
from investor_agent.data_engine import NSESTORE
from investor_agent.logger import get_logger

logger = get_logger(__name__)

# Lazy-loaded resources for semantic search
try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    from google.adk.tools import tool
    from dateutil.relativedelta import relativedelta
    _SEMANTIC_SEARCH_AVAILABLE = True
except ImportError:
    _SEMANTIC_SEARCH_AVAILABLE = False
    logger.warning("chromadb or sentence-transformers not installed - semantic_search will be unavailable")
    # Create a dummy decorator if ADK not available
    def tool(func):
        return func
    # Dummy relativedelta if dateutil not available
    class relativedelta:
        def __init__(self, **kwargs):
            pass

# State for semantic search resources (lazy initialization)
_search_state = SimpleNamespace(collections=[], model=None, initialized=False)

# Sector mapping for major Indian stocks
# This is a simplified mapping - expand as needed
SECTOR_MAP = {
    # Banking
    "HDFCBANK": "Banking", "ICICIBANK": "Banking", "SBIN": "Banking", 
    "AXISBANK": "Banking", "KOTAKBANK": "Banking", "INDUSINDBK": "Banking",
    "BANDHANBNK": "Banking", "FEDERALBNK": "Banking", "IDFCFIRSTB": "Banking",
    "PNB": "Banking", "BANKBARODA": "Banking", "CANBK": "Banking",
    
    # IT
    "TCS": "IT", "INFY": "IT", "WIPRO": "IT", "HCLTECH": "IT",
    "TECHM": "IT", "LTIM": "IT", "COFORGE": "IT", "MPHASIS": "IT",
    
    # Auto
    "MARUTI": "Auto", "M&M": "Auto", "TATAMOTORS": "Auto", "BAJAJ-AUTO": "Auto",
    "EICHERMOT": "Auto", "HEROMOTOCO": "Auto", "TVSMOTOR": "Auto",
    
    # Pharma
    "SUNPHARMA": "Pharma", "DRREDDY": "Pharma", "CIPLA": "Pharma",
    "DIVISLAB": "Pharma", "BIOCON": "Pharma", "AUROPHARMA": "Pharma",
    
    # FMCG
    "HINDUNILVR": "FMCG", "ITC": "FMCG", "NESTLEIND": "FMCG",
    "BRITANNIA": "FMCG", "DABUR": "FMCG", "MARICO": "FMCG",
    
    # Energy & Oil
    "RELIANCE": "Energy", "ONGC": "Energy", "BPCL": "Energy",
    "IOC": "Energy", "HINDPETRO": "Energy", "ADANIGREEN": "Energy",
    
    # Metals
    "TATASTEEL": "Metals", "HINDALCO": "Metals", "JSWSTEEL": "Metals",
    "VEDL": "Metals", "COALINDIA": "Metals", "SAIL": "Metals",
    
    # Telecom
    "BHARTIARTL": "Telecom", "IDEA": "Telecom",
    
    # Financial Services (NBFCs)
    "BAJFINANCE": "Financial Services", "SHRIRAMFIN": "Financial Services",
    "CHOLAFIN": "Financial Services", "MUTHOOTFIN": "Financial Services",
    "LICHSGFIN": "Financial Services", "HDFCLIFE": "Financial Services",
    "SBILIFE": "Financial Services", "ICICIGI": "Financial Services",
}


def get_sector_stocks(sector: str) -> list:
    """Get list of stock symbols for a given sector."""
    sector_upper = sector.upper()
    return [symbol for symbol, sec in SECTOR_MAP.items() if sec.upper() == sector_upper]


def _parse_date(date_str: Optional[str]) -> Optional[date]:
    """
    Safely parse a date string in YYYY-MM-DD format.
    Returns a `date` or `None` if input is None or invalid.
    """
    if not date_str:
        logger.debug("_parse_date: no date_str provided, returning None")
        return None
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except Exception as e:
        logger.warning(f"_parse_date: failed to parse '{date_str}': {e}")
        return None


def check_data_availability() -> str:
    """
    Returns the start and end dates of the available data in the database.
    ALWAYS call this FIRST to understand what data is available.
    
    This tells you:
    - What 'Today', 'Yesterday', or 'Last Week' means in context of this data
    - The actual date range you can query
    """
    # Trigger load if not loaded
    _ = NSESTORE.df
    
    if NSESTORE.min_date and NSESTORE.max_date:
        return f"""Data Availability Report:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“… Start Date: {NSESTORE.min_date}
ðŸ“… End Date:   {NSESTORE.max_date}
ðŸ“Š Total Symbols: {NSESTORE.total_symbols:,}
ðŸ“ˆ Total Records: {len(NSESTORE.df):,}

Use these dates as reference for all queries.
For 'latest week', use the 7 days ending on {NSESTORE.max_date}."""
    
    return "âš ï¸ No data currently loaded."


def get_top_gainers(start_date: Optional[str] = None, end_date: Optional[str] = None, top_n: int = 10) -> dict:
    """
    Get top performing stocks by percentage return over a period.
    
    Args:
        start_date: Start date in YYYY-MM-DD format (optional)
        end_date: End date in YYYY-MM-DD format (optional)
        top_n: Number of top stocks to return (default 10)
    
    Returns:
        Dictionary with period info, top gainers list, and summary statistics
    
    If dates are not provided, defaults to the last 7 days of available data.
    """
    _ = NSESTORE.df  # Ensure data loaded
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    
    # Default to last 7 days if no dates provided
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=7)
            dates_defaulted = True
        else:
            return {"error": "No data available", "gainers": [], "period": {}}
    
    # Get ranked stocks
    ranked = NSESTORE.get_ranked_stocks(s_date, e_date, top_n=top_n, metric="return")
    
    if ranked.empty:
        return {
            "error": f"No data found between {s_date} and {e_date}",
            "gainers": [],
            "period": {"start": str(s_date), "end": str(e_date)}
        }
    
    # Build structured output
    return {
        "tool": "get_top_gainers",
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(ranked.iloc[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "gainers": [
            {
                "rank": idx + 1,
                "symbol": row['symbol'],
                "return_pct": round(float(row['return_pct']), 2),
                "price_start": round(float(row['start_price']), 2),
                "price_end": round(float(row['end_price']), 2),
                "volatility": round(float(row['volatility']), 2),
                "delivery_pct": round(float(row['avg_delivery_pct']), 1) if row['avg_delivery_pct'] else None
            }
            for idx, row in ranked.iterrows()
        ],
        "summary": {
            "avg_return": round(float(ranked['return_pct'].mean()), 2),
            "top_symbol": ranked.iloc[0]['symbol'],
            "top_return": round(float(ranked.iloc[0]['return_pct']), 2),
            "count": len(ranked)
        }
    }


def get_top_losers(start_date: Optional[str] = None, end_date: Optional[str] = None, top_n: int = 10) -> dict:
    """
    Get worst performing stocks by percentage return over a period.
    
    Args:
        start_date: Start date in YYYY-MM-DD format (optional)
        end_date: End date in YYYY-MM-DD format (optional)
        top_n: Number of bottom stocks to return (default 10)
    
    Returns:
        Dictionary with period info, top losers list, and summary statistics
    
    If dates are not provided, defaults to the last 7 days of available data.
    """
    _ = NSESTORE.df  # Ensure data loaded
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    
    # Default to last 7 days if no dates provided
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=7)
            dates_defaulted = True
        else:
            return {"error": "No data available", "losers": [], "period": {}}
    
    # Get all ranked stocks and take bottom N
    all_ranked = NSESTORE.get_ranked_stocks(s_date, e_date, top_n=1000, metric="return")
    
    if all_ranked.empty:
        return {
            "error": f"No data found between {s_date} and {e_date}",
            "losers": [],
            "period": {"start": str(s_date), "end": str(e_date)}
        }
    
    # Get bottom performers
    losers = all_ranked.tail(top_n).sort_values("return_pct")
    
    return {
        "tool": "get_top_losers",
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(losers.iloc[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "losers": [
            {
                "rank": idx + 1,
                "symbol": row['symbol'],
                "return_pct": round(float(row['return_pct']), 2),
                "price_start": round(float(row['start_price']), 2),
                "price_end": round(float(row['end_price']), 2),
                "volatility": round(float(row['volatility']), 2),
                "delivery_pct": round(float(row['avg_delivery_pct']), 1) if row['avg_delivery_pct'] else None
            }
            for idx, row in losers.iterrows()
        ],
        "summary": {
            "avg_return": round(float(losers['return_pct'].mean()), 2),
            "worst_symbol": losers.iloc[0]['symbol'],
            "worst_return": round(float(losers.iloc[0]['return_pct']), 2),
            "count": len(losers)
        }
    }


def get_sector_top_performers(
    sector: str, 
    start_date: Optional[str] = None, 
    end_date: Optional[str] = None, 
    top_n: int = 5
) -> dict:
    """
    Get top performing stocks from a specific sector.
    
    Args:
        sector: Sector name (e.g., 'Banking', 'IT', 'Auto', 'Pharma', 'FMCG', 
                'Energy', 'Metals', 'Telecom', 'Financial Services')
        start_date: Start date in YYYY-MM-DD format (optional)
        end_date: End date in YYYY-MM-DD format (optional)
        top_n: Number of top stocks to return (default 5)
    
    Returns:
        Dictionary with sector performers, period info, and summary statistics
    
    Available sectors: Banking, IT, Auto, Pharma, FMCG, Energy, Metals, 
                      Telecom, Financial Services
    """
    # Get stocks in this sector
    sector_stocks = get_sector_stocks(sector)
    
    if not sector_stocks:
        available_sectors = sorted(set(SECTOR_MAP.values()))
        return {
            "tool": "get_sector_top_performers",
            "error": f"Sector '{sector}' not found. Available: {', '.join(available_sectors)}"
        }
    
    _ = NSESTORE.df
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=30)
            dates_defaulted = True
        else:
            return {"tool": "get_sector_top_performers", "error": "No data available"}
    
    # Analyze each stock in the sector
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol in sector_stocks:
        stock_df = NSESTORE.get_stock_data(symbol, s_date, e_date)
        if not stock_df.empty:
            stats = MetricsEngine.calculate_period_stats(stock_df)
            if stats:
                stats['symbol'] = symbol
                results.append(stats)
    
    if not results:
        return {
            "tool": "get_sector_top_performers",
            "error": f"No data found for {sector} stocks between {s_date} and {e_date}"
        }
    
    # Sort by return percentage
    results.sort(key=lambda x: x['return_pct'], reverse=True)
    results = results[:top_n]
    
    return {
        "tool": "get_sector_top_performers",
        "sector": sector,
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(results[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "performers": [
            {
                "rank": idx + 1,
                "symbol": stats['symbol'],
                "return_pct": round(float(stats['return_pct']), 2),
                "price_start": round(float(stats['start_price']), 2),
                "price_end": round(float(stats['end_price']), 2),
                "volatility": round(float(stats['volatility']), 2),
                "delivery_pct": round(float(stats['avg_delivery_pct']), 1) if stats['avg_delivery_pct'] else None
            }
            for idx, stats in enumerate(results)
        ],
        "summary": {
            "sector_avg_return": round(sum(s['return_pct'] for s in results) / len(results), 2),
            "stocks_analyzed": len(results),
            "total_sector_stocks": len(sector_stocks),
            "top_symbol": results[0]['symbol'],
            "top_return": round(float(results[0]['return_pct']), 2)
        }
    }


def analyze_stock(symbol: str, start_date: Optional[str] = None, end_date: Optional[str] = None) -> dict:
    """
    Comprehensive analysis of a single stock over a period.
    
    Args:
        symbol: Stock symbol (e.g., 'RELIANCE', 'TCS')
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format
    
    Returns:
        Dictionary with comprehensive stock analysis including price, technical, risk, and momentum metrics
    
    If dates not provided, analyzes the last 30 days of available data.
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=30)
            dates_defaulted = True
        else:
            return {"tool": "analyze_stock", "error": "No data available"}
    
    # Get stock data
    stock_df = NSESTORE.get_stock_data(symbol.upper(), s_date, e_date)
    
    if stock_df.empty:
        return {
            "tool": "analyze_stock",
            "error": f"No data found for {symbol.upper()} between {s_date} and {e_date}"
        }
    
    # Calculate metrics
    from investor_agent.data_engine import MetricsEngine
    stats = MetricsEngine.calculate_period_stats(stock_df)
    
    if not stats:
        return {"tool": "analyze_stock", "error": f"Insufficient data to analyze {symbol.upper()}"}
    
    # Calculate additional metrics
    price_range_pct = ((stats['period_high'] - stats['period_low']) / stats['start_price'] * 100)
    sma20_distance = ((stats['end_price'] / stats['sma_20'] - 1) * 100) if stats['sma_20'] > 0 else 0
    sma50_distance = ((stats['end_price'] / stats['sma_50'] - 1) * 100) if stats['sma_50'] > 0 else 0
    
    # Determine verdict
    verdict = "Neutral"
    verdict_reason = "Sideways movement, wait for clear trend"
    
    if stats['return_pct'] > 5 and stats['avg_delivery_pct'] > 60:
        verdict = "Strong Accumulation"
        verdict_reason = "High returns with high delivery suggests institutional buying"
    elif stats['return_pct'] > 3 and stats['avg_delivery_pct'] > 50:
        verdict = "Positive Momentum"
        verdict_reason = "Good returns with decent delivery"
    elif stats['return_pct'] < -5 and stats['avg_delivery_pct'] > 60:
        verdict = "Distribution Pattern"
        verdict_reason = "Falling price with high delivery suggests selling pressure"
    elif stats['return_pct'] < -3:
        verdict = "Weakness"
        verdict_reason = "Negative returns, proceed with caution"
    elif stats['volatility'] > 10:
        verdict = "High Volatility"
        verdict_reason = "Significant price swings, suitable for traders not investors"
    
    # Determine trend
    if stats['end_price'] > stats['sma_20'] > stats['sma_50']:
        trend = "UPTREND"
        trend_detail = "Price above both SMAs"
    elif stats['end_price'] < stats['sma_20'] < stats['sma_50']:
        trend = "DOWNTREND"
        trend_detail = "Price below both SMAs"
    else:
        trend = "SIDEWAYS"
        trend_detail = "Mixed signals"
    
    return {
        "tool": "analyze_stock",
        "symbol": symbol.upper(),
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(stats['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "price": {
            "start": round(float(stats['start_price']), 2),
            "end": round(float(stats['end_price']), 2),
            "high": round(float(stats['period_high']), 2),
            "low": round(float(stats['period_low']), 2),
            "return_pct": round(float(stats['return_pct']), 2),
            "momentum_pct": round(float(stats['momentum_pct']), 2),
            "range_pct": round(float(price_range_pct), 2)
        },
        "technical": {
            "sma_20": round(float(stats['sma_20']), 2),
            "sma_50": round(float(stats['sma_50']), 2),
            "sma20_distance_pct": round(float(sma20_distance), 1),
            "sma50_distance_pct": round(float(sma50_distance), 1),
            "distance_from_high_pct": round(float(stats['distance_from_high_pct']), 1),
            "distance_from_low_pct": round(float(stats['distance_from_low_pct']), 1)
        },
        "risk": {
            "volatility": round(float(stats['volatility']), 2),
            "max_drawdown": round(float(stats['max_drawdown']), 2),
            "stability": "High" if stats['volatility'] < 2 else "Moderate" if stats['volatility'] < 5 else "Low"
        },
        "momentum": {
            "consecutive_up_days": int(stats['consecutive_ups']),
            "consecutive_down_days": int(stats['consecutive_downs']),
            "volume_trend_pct": round(float(stats['volume_trend_pct']), 1)
        },
        "volume": {
            "avg_daily_volume": int(stats['avg_volume']),
            "avg_delivery_pct": round(float(stats['avg_delivery_pct']), 1)
        },
        "verdict": {
            "signal": verdict,
            "reason": verdict_reason,
            "trend": trend,
            "trend_detail": trend_detail
        }
    }


# ==============================================================================
# PHASE 2: ADVANCED ANALYSIS TOOLS
# ==============================================================================

def detect_volume_surge(symbol: str, lookback_days: int = 20) -> dict:
    """
    Detect unusual volume activity by comparing recent volume to historical average.
    
    Args:
        symbol: Stock symbol (e.g., 'RELIANCE', 'TCS')
        lookback_days: Number of days to use for average calculation (default 20)
    
    Returns:
        Dictionary with volume analysis showing if current volume is significantly higher than average
        (indicates potential breakout, news event, or institutional activity)
    """
    _ = NSESTORE.df
    
    if not NSESTORE.max_date:
        return {"tool": "detect_volume_surge", "error": "No data available"}
    
    end_date = NSESTORE.max_date
    start_date = end_date - timedelta(days=lookback_days + 5)  # Extra buffer
    
    stock_df = NSESTORE.get_stock_data(symbol.upper(), start_date, end_date)
    
    if stock_df.empty or len(stock_df) < 5:
        return {
            "tool": "detect_volume_surge",
            "error": f"Insufficient data for {symbol.upper()}"
        }
    
    # Get recent volume (last 3 days avg)
    recent_vol = stock_df.tail(3)['VOLUME'].mean()
    
    # Get baseline average (exclude last 3 days)
    baseline_vol = stock_df.iloc[:-3]['VOLUME'].mean()
    
    if baseline_vol == 0:
        return {
            "tool": "detect_volume_surge",
            "error": f"Invalid volume data for {symbol.upper()}"
        }
    
    surge_pct = ((recent_vol - baseline_vol) / baseline_vol) * 100
    
    # Determine verdict
    if surge_pct > 100:
        verdict = "EXTREME SURGE"
        interpretation = "Volume doubled! Possible news/event catalyst"
    elif surge_pct > 50:
        verdict = "HIGH SURGE"
        interpretation = "Significant volume increase, watch for breakout"
    elif surge_pct > 20:
        verdict = "MODERATE SURGE"
        interpretation = "Above-average interest"
    elif surge_pct < -20:
        verdict = "LOW VOLUME"
        interpretation = "Below average, consolidation or lack of interest"
    else:
        verdict = "NORMAL"
        interpretation = "Volume within typical range"
    
    return {
        "tool": "detect_volume_surge",
        "symbol": symbol.upper(),
        "period": {
            "lookback_days": lookback_days,
            "end_date": str(end_date)
        },
        "volume": {
            "recent_avg": int(recent_vol),
            "baseline_avg": int(baseline_vol),
            "surge_pct": round(float(surge_pct), 1)
        },
        "verdict": verdict,
        "interpretation": interpretation
    }


def compare_stocks(symbols: list[str], start_date: Optional[str] = None, end_date: Optional[str] = None) -> dict:
    """
    Side-by-side comparison of multiple stocks over the same period.
    
    Args:
        symbols: List of stock symbols (e.g., ['RELIANCE', 'TCS', 'HDFCBANK'])
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format
    
    Returns:
        Dictionary with comparative analysis of all stocks
        
    If dates not provided, uses last 30 days.
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=30)
            dates_defaulted = True
        else:
            return {"tool": "compare_stocks", "error": "No data available"}
    
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol in symbols:
        stock_df = NSESTORE.get_stock_data(symbol.upper(), s_date, e_date)
        if not stock_df.empty:
            stats = MetricsEngine.calculate_period_stats(stock_df)
            if stats:
                stats['symbol'] = symbol.upper()
                results.append(stats)
    
    if not results:
        return {
            "tool": "compare_stocks",
            "error": f"No data found for any symbols between {s_date} and {e_date}"
        }
    
    # Determine verdict for each stock
    comparisons = []
    for stats in results:
        if stats['return_pct'] > 5:
            verdict = "Strong"
        elif stats['return_pct'] > 0:
            verdict = "Positive"
        elif stats['return_pct'] > -5:
            verdict = "Weak"
        else:
            verdict = "Poor"
        
        comparisons.append({
            "symbol": stats['symbol'],
            "return_pct": round(float(stats['return_pct']), 2),
            "volatility": round(float(stats['volatility']), 2),
            "delivery_pct": round(float(stats['avg_delivery_pct']), 1),
            "price_start": round(float(stats['start_price']), 2),
            "price_end": round(float(stats['end_price']), 2),
            "verdict": verdict
        })
    
    # Find best and worst performers
    best = max(results, key=lambda x: x['return_pct'])
    worst = min(results, key=lambda x: x['return_pct'])
    
    return {
        "tool": "compare_stocks",
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(results[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "comparisons": comparisons,
        "summary": {
            "best_performer": best['symbol'],
            "best_return": round(float(best['return_pct']), 2),
            "worst_performer": worst['symbol'],
            "worst_return": round(float(worst['return_pct']), 2),
            "spread": round(float(best['return_pct'] - worst['return_pct']), 2),
            "stocks_compared": len(comparisons)
        }
    }


def get_delivery_momentum(start_date: Optional[str] = None, end_date: Optional[str] = None, min_delivery: float = 50.0) -> str:
    """
    Find stocks with consistently high delivery percentage (institutional buying).
    
    Args:
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format
        min_delivery: Minimum average delivery % threshold (default 50%)
    
    Returns:
        List of stocks showing strong institutional interest
        
    High delivery % (>50%) indicates institutions are taking delivery, not just trading.
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    # Default to last 14 days
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=14)
        else:
            return "âŒ No data available."
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(s_date)) & (df["DATE"] <= pd.Timestamp(e_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return f"âŒ No data found between {s_date} and {e_date}"
    
    # Calculate average delivery for each stock
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol, group in filtered.groupby("SYMBOL"):
        stats = MetricsEngine.calculate_period_stats(group)
        if stats and stats['avg_delivery_pct'] >= min_delivery:
            stats['symbol'] = symbol
            results.append(stats)
    
    if not results:
        return f"âŒ No stocks found with delivery % >= {min_delivery}%"
    
    # Sort by delivery percentage (highest first)
    results.sort(key=lambda x: x['avg_delivery_pct'], reverse=True)
    results = results[:15]  # Top 15
    
    output = f"""### ðŸ¦ High Delivery Momentum ({s_date} to {e_date})

Stocks with avg delivery â‰¥ {min_delivery}% (institutional conviction)

| Rank | Symbol | Delivery % | Return % | Price Trend | Signal |
|------|--------|------------|----------|-------------|--------|
"""
    
    for idx, stats in enumerate(results, 1):
        # Determine signal
        if stats['return_pct'] > 5 and stats['avg_delivery_pct'] > 60:
            signal = "ðŸŸ¢ Strong Buy"
        elif stats['return_pct'] > 0 and stats['avg_delivery_pct'] > 50:
            signal = "ðŸŸ¢ Accumulation"
        elif stats['return_pct'] < -5 and stats['avg_delivery_pct'] > 60:
            signal = "ðŸ”´ Distribution"
        else:
            signal = "ðŸŸ¡ Watch"
        
        output += (f"| {idx:2d}   | {stats['symbol']:10s} | "
                  f"{stats['avg_delivery_pct']:5.1f}% | {stats['return_pct']:+6.2f}% | "
                  f"â‚¹{stats['start_price']:.2f}â†’â‚¹{stats['end_price']:.2f} | {signal} |\n")
    
    output += f"\n**Total stocks with high delivery:** {len(results)}\n"
    output += "**Interpretation:** High delivery % = Institutions taking positions (bullish if price rising)\n"
    
    return output


def get_delivery_momentum(start_date: Optional[str] = None, end_date: Optional[str] = None, min_delivery: float = 50.0) -> dict:
    """
    Find stocks with consistently high delivery percentage (institutional buying).
    
    Args:
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format
        min_delivery: Minimum average delivery % threshold (default 50%)
    
    Returns:
        Dictionary with list of stocks showing strong institutional interest
        
    High delivery % (>50%) indicates institutions are taking delivery, not just trading.
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=14)
            dates_defaulted = True
        else:
            return {"tool": "get_delivery_momentum", "error": "No data available"}
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(s_date)) & (df["DATE"] <= pd.Timestamp(e_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return {
            "tool": "get_delivery_momentum",
            "error": f"No data found between {s_date} and {e_date}"
        }
    
    # Calculate average delivery for each stock
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol, group in filtered.groupby("SYMBOL"):
        stats = MetricsEngine.calculate_period_stats(group)
        if stats and stats['avg_delivery_pct'] >= min_delivery:
            stats['symbol'] = symbol
            results.append(stats)
    
    if not results:
        return {
            "tool": "get_delivery_momentum",
            "error": f"No stocks found with delivery % >= {min_delivery}%"
        }
    
    # Sort by delivery percentage (highest first)
    results.sort(key=lambda x: x['avg_delivery_pct'], reverse=True)
    results = results[:15]  # Top 15
    
    stocks = []
    for idx, stats in enumerate(results, 1):
        # Determine signal
        if stats['return_pct'] > 5 and stats['avg_delivery_pct'] > 60:
            signal = "Strong Buy"
        elif stats['return_pct'] > 0 and stats['avg_delivery_pct'] > 50:
            signal = "Accumulation"
        elif stats['return_pct'] < -5 and stats['avg_delivery_pct'] > 60:
            signal = "Distribution"
        else:
            signal = "Watch"
        
        stocks.append({
            "rank": idx,
            "symbol": stats['symbol'],
            "delivery_pct": round(float(stats['avg_delivery_pct']), 1),
            "return_pct": round(float(stats['return_pct']), 2),
            "price_start": round(float(stats['start_price']), 2),
            "price_end": round(float(stats['end_price']), 2),
            "signal": signal
        })
    
    return {
        "tool": "get_delivery_momentum",
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(results[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "min_delivery_threshold": min_delivery,
        "stocks": stocks,
        "summary": {
            "total_found": len(stocks),
            "avg_delivery": round(sum(s['delivery_pct'] for s in stocks) / len(stocks), 1),
            "interpretation": "High delivery % = Institutions taking positions (bullish if price rising)"
        }
    }


def detect_breakouts(start_date: Optional[str] = None, end_date: Optional[str] = None, threshold: float = 10.0) -> dict:
    """
    Detect stocks that are breaking out (hitting new highs with strong momentum).
    
    Args:
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format  
        threshold: Minimum return % to qualify as breakout (default 10%)
    
    Returns:
        Dictionary with list of stocks showing price breakouts and strong momentum
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=7)
            dates_defaulted = True
        else:
            return {"tool": "detect_breakouts", "error": "No data available"}
    
    # Get top gainers
    ranked = NSESTORE.get_ranked_stocks(s_date, e_date, top_n=50, metric="return")
    
    if ranked.empty:
        return {
            "tool": "detect_breakouts",
            "error": f"No data found between {s_date} and {e_date}"
        }
    
    # Filter for breakout candidates (high return + moderate volatility)
    breakouts_df = ranked[
        (ranked['return_pct'] >= threshold) & 
        (ranked['volatility'] < 15)  # Not too volatile (avoid manipulation)
    ].head(10)
    
    if breakouts_df.empty:
        return {
            "tool": "detect_breakouts",
            "error": f"No breakout candidates found (return >= {threshold}%, volatility < 15%)"
        }
    
    breakouts = []
    for idx, (_, row) in enumerate(breakouts_df.iterrows(), 1):
        # Quality score
        if row['avg_delivery_pct'] > 60:
            quality = "High (Institutional)"
        elif row['avg_delivery_pct'] > 40:
            quality = "Medium"
        else:
            quality = "Low (Retail)"
        
        breakouts.append({
            "rank": idx,
            "symbol": row['symbol'],
            "return_pct": round(float(row['return_pct']), 2),
            "volatility": round(float(row['volatility']), 2),
            "delivery_pct": round(float(row['avg_delivery_pct']), 1),
            "price_start": round(float(row['start_price']), 2),
            "price_end": round(float(row['end_price']), 2),
            "quality": quality
        })
    
    return {
        "tool": "detect_breakouts",
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(breakouts_df.iloc[0]['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "threshold": threshold,
        "breakouts": breakouts,
        "summary": {
            "total_found": len(breakouts),
            "avg_return": round(sum(b['return_pct'] for b in breakouts) / len(breakouts), 2),
            "strategy": "Look for high delivery % breakouts (institutional backing)"
        }
    }


def list_available_tools() -> str:
    """
    Lists all available analysis tools with brief descriptions.
    Use this when user asks 'what can you do?' or 'what tools do you have?'
    
    Returns:
        Formatted list of all available tools and their purposes
    """
    return """### ðŸ› ï¸ Available Analysis Tools

**PHASE 1: Core Stock Analysis**

1ï¸âƒ£ **check_data_availability()**
   â””â”€ Get date range and database statistics
   
2ï¸âƒ£ **get_top_gainers(start_date, end_date, top_n)**
   â””â”€ Find best performing stocks by return %
   
3ï¸âƒ£ **get_top_losers(start_date, end_date, top_n)**
   â””â”€ Find worst performing stocks by return %
   
4ï¸âƒ£ **get_sector_top_performers(sector, start_date, end_date, top_n)** ðŸ†•
   â””â”€ Get top stocks from specific sector (Banking, IT, Auto, Pharma, FMCG, etc.)
   
5ï¸âƒ£ **analyze_stock(symbol, start_date, end_date)**
   â””â”€ Deep-dive analysis of individual stock with comprehensive metrics

**PHASE 2: Advanced Pattern Detection**

6ï¸âƒ£ **detect_volume_surge(symbol, lookback_days)**
   â””â”€ Identify unusual volume activity (potential breakouts/news events)
   
7ï¸âƒ£ **compare_stocks(symbols, start_date, end_date)**
   â””â”€ Side-by-side comparison of multiple stocks
   
8ï¸âƒ£ **get_delivery_momentum(start_date, end_date, min_delivery)**
   â””â”€ Find stocks with high institutional buying (delivery %)
   
9ï¸âƒ£ **detect_breakouts(start_date, end_date, threshold)**
   â””â”€ Identify momentum stocks breaking out with strong signals

**PHASE 3: Professional Trading Tools**

ðŸ”Ÿ **get_52week_high_low(symbols, top_n)**
   â””â”€ Find stocks near 52-week highs (breakouts) or lows (reversals)
   
1ï¸âƒ£1ï¸âƒ£ **analyze_risk_metrics(symbol, start_date, end_date)**
   â””â”€ Advanced risk analysis: max drawdown, Sharpe ratio, volatility trends
   
1ï¸âƒ£2ï¸âƒ£ **find_momentum_stocks(min_return, min_consecutive_days, top_n)**
   â””â”€ Find stocks with strong upward momentum (consecutive up days)
   
1ï¸âƒ£3ï¸âƒ£ **detect_reversal_candidates(lookback_days, top_n)**
   â””â”€ Find oversold stocks showing early reversal signals
   
1ï¸âƒ£4ï¸âƒ£ **get_volume_price_divergence(min_divergence, top_n)**
   â””â”€ Detect bearish/bullish divergence between price and volume

**NEWS & SYNTHESIS**

1ï¸âƒ£5ï¸âƒ£ **google_search(query)** [News Agent]
   â””â”€ Search financial news to correlate with price movements

**AVAILABLE SECTORS FOR FILTERING:**
ðŸ¦ Banking, ðŸ’» IT, ðŸš— Auto, ðŸ’Š Pharma, ðŸ›’ FMCG, 
âš¡ Energy, ðŸ­ Metals, ðŸ“± Telecom, ðŸ’° Financial Services

**ADVANCED METRICS AVAILABLE:**
ðŸ“Š Max Drawdown, Sharpe Ratio, Win Rate
ðŸ“ˆ Moving Averages (20-day, 50-day SMA)
ðŸŽ¯ 52-Week High/Low, Support/Resistance Levels
âš¡ Momentum Indicators, Consecutive Days Streaks
ðŸ“‰ Volume-Price Divergence, Volume Trends
ðŸ” Risk-Adjusted Returns, Downside Volatility

**DATA COVERAGE:**
ðŸ“… NSE stock data with OHLCV + delivery metrics
ðŸ“Š Multiple years of historical data for trend analysis
ðŸ”¬ Real-time pattern detection with professional trading algorithms

**TIP:** Always start with check_data_availability() to understand the date range!
"""


# ==============================================================================
# PHASE 3: PROFESSIONAL TRADING TOOLS
# ==============================================================================

def get_52week_high_low(symbols: Optional[list[str]] = None, top_n: int = 20) -> dict:
    """
    Find stocks near their 52-week highs or lows (critical psychological levels).
    
    Args:
        symbols: Optional list of symbols to check (if None, scans all stocks)
        top_n: Number of stocks to return (default 20)
    
    Returns:
        Dictionary with stocks trading near 52-week highs (breakout candidates) or lows (reversal plays)
        
    Near 52-week high = within 5% of high (bullish breakout)
    Near 52-week low = within 10% of low (potential reversal/value play)
    """
    _ = NSESTORE.df
    
    if not NSESTORE.max_date:
        return {"tool": "get_52week_high_low", "error": "No data available"}
    
    end_date = NSESTORE.max_date
    start_date = end_date - timedelta(days=365)
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(start_date)) & (df["DATE"] <= pd.Timestamp(end_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return {"tool": "get_52week_high_low", "error": "Insufficient data for 52-week analysis"}
    
    near_highs = []
    near_lows = []
    
    symbols_to_check = symbols if symbols else filtered['SYMBOL'].unique()
    
    for symbol in symbols_to_check:
        stock_df = filtered[filtered['SYMBOL'] == symbol]
        if stock_df.empty:
            continue
        
        week_52_high = stock_df['HIGH'].max()
        week_52_low = stock_df['LOW'].min()
        current_price = stock_df.iloc[-1]['CLOSE']
        
        # Distance from 52-week high/low
        dist_from_high = ((current_price - week_52_high) / week_52_high) * 100
        dist_from_low = ((current_price - week_52_low) / week_52_low) * 100
        
        # Near 52-week high (within 5%)
        if dist_from_high >= -5:
            signal = "At High" if dist_from_high >= -1 else "Near High"
            near_highs.append({
                'symbol': symbol,
                'current_price': round(float(current_price), 2),
                'week_52_high': round(float(week_52_high), 2),
                'distance_pct': round(float(dist_from_high), 2),
                'signal': signal
            })
        
        # Near 52-week low (within 10%)
        if dist_from_low <= 10:
            signal = "At Low" if dist_from_low <= 1 else "Near Low"
            near_lows.append({
                'symbol': symbol,
                'current_price': round(float(current_price), 2),
                'week_52_low': round(float(week_52_low), 2),
                'distance_pct': round(float(dist_from_low), 2),
                'signal': signal
            })
    
    # Sort and limit
    near_highs.sort(key=lambda x: x['distance_pct'], reverse=True)
    near_lows.sort(key=lambda x: x['distance_pct'])
    
    return {
        "tool": "get_52week_high_low",
        "period": {
            "start": str(start_date),
            "end": str(end_date),
            "days": 365
        },
        "near_highs": near_highs[:top_n],
        "near_lows": near_lows[:top_n],
        "summary": {
            "stocks_near_high": len(near_highs),
            "stocks_near_low": len(near_lows),
            "strategy": "52W High breakouts need volume confirmation + delivery >50%; 52W Low reversals need positive divergence"
        }
    }


def analyze_risk_metrics(symbol: str, start_date: Optional[str] = None, end_date: Optional[str] = None) -> dict:
    """
    Advanced risk analysis for a stock: max drawdown, Sharpe-like metrics, volatility trends.
    
    Args:
        symbol: Stock symbol
        start_date: Optional start date in YYYY-MM-DD format
        end_date: Optional end date in YYYY-MM-DD format
    
    Returns:
        Dictionary with comprehensive risk assessment including max drawdown, volatility analysis, and risk-adjusted returns
    """
    _ = NSESTORE.df
    
    s_date = _parse_date(start_date)
    e_date = _parse_date(end_date)
    
    dates_defaulted = False
    if not s_date or not e_date:
        if NSESTORE.max_date:
            e_date = NSESTORE.max_date
            s_date = e_date - timedelta(days=90)
            dates_defaulted = True
        else:
            return {"tool": "analyze_risk_metrics", "error": "No data available"}
    
    stock_df = NSESTORE.get_stock_data(symbol.upper(), s_date, e_date)
    
    if stock_df.empty or len(stock_df) < 10:
        return {
            "tool": "analyze_risk_metrics",
            "error": f"Insufficient data for {symbol.upper()}"
        }
    
    from investor_agent.data_engine import MetricsEngine
    stats = MetricsEngine.calculate_period_stats(stock_df)
    
    if not stats:
        return {
            "tool": "analyze_risk_metrics",
            "error": f"Unable to calculate metrics for {symbol.upper()}"
        }
    
    # Calculate additional risk metrics
    daily_returns = stock_df['CLOSE'].pct_change().dropna()
    
    # Sharpe-like ratio (return / volatility)
    risk_adjusted_return = stats['return_pct'] / stats['volatility'] if stats['volatility'] > 0 else 0
    
    # Downside volatility (only negative returns)
    downside_returns = daily_returns[daily_returns < 0]
    downside_volatility = downside_returns.std() * 100 if len(downside_returns) > 0 else 0
    
    # Win rate (percentage of positive days)
    positive_days = len(daily_returns[daily_returns > 0])
    win_rate = (positive_days / len(daily_returns) * 100) if len(daily_returns) > 0 else 0
    
    # Risk verdict
    if abs(stats['max_drawdown']) > 20:
        risk_level = "HIGH RISK"
        risk_detail = "Severe drawdowns observed (>20%)"
    elif abs(stats['max_drawdown']) > 10:
        risk_level = "MODERATE RISK"
        risk_detail = "Significant volatility (10-20% drawdown)"
    else:
        risk_level = "LOW RISK"
        risk_detail = "Stable performance (<10% drawdown)"
    
    # Sharpe interpretation
    if risk_adjusted_return > 1.5:
        sharpe_rating = "EXCELLENT"
        sharpe_detail = "risk-reward ratio (>1.5x)"
    elif risk_adjusted_return > 0.8:
        sharpe_rating = "GOOD"
        sharpe_detail = "risk-reward ratio (0.8-1.5x)"
    elif risk_adjusted_return > 0:
        sharpe_rating = "FAIR"
        sharpe_detail = "risk-reward ratio (0-0.8x)"
    else:
        sharpe_rating = "POOR"
        sharpe_detail = "losses exceed volatility"
    
    # Trend assessment
    if stats['end_price'] > stats['sma_20'] > stats['sma_50']:
        trend = "UPTREND"
        trend_detail = "Price above both 20-day and 50-day SMA"
    elif stats['end_price'] < stats['sma_20'] < stats['sma_50']:
        trend = "DOWNTREND"
        trend_detail = "Price below both SMAs"
    else:
        trend = "MIXED TREND"
        trend_detail = "Consolidation phase"
    
    return {
        "tool": "analyze_risk_metrics",
        "symbol": symbol.upper(),
        "period": {
            "start": str(s_date),
            "end": str(e_date),
            "days": int(stats['days_count']),
            "dates_defaulted": dates_defaulted
        },
        "returns": {
            "total_return_pct": round(float(stats['return_pct']), 2),
            "momentum_pct": round(float(stats['momentum_pct']), 2),
            "risk_adjusted_return": round(float(risk_adjusted_return), 2)
        },
        "risk": {
            "max_drawdown": round(float(stats['max_drawdown']), 2),
            "volatility": round(float(stats['volatility']), 2),
            "downside_volatility": round(float(downside_volatility), 2),
            "win_rate": round(float(win_rate), 1),
            "positive_days": int(positive_days),
            "total_days": len(daily_returns)
        },
        "technical": {
            "current_price": round(float(stats['end_price']), 2),
            "sma_20": round(float(stats['sma_20']), 2),
            "sma_50": round(float(stats['sma_50']), 2),
            "sma20_distance_pct": round(((stats['end_price']/stats['sma_20']-1)*100), 1) if stats['sma_20'] > 0 else 0,
            "sma50_distance_pct": round(((stats['end_price']/stats['sma_50']-1)*100), 1) if stats['sma_50'] > 0 else 0,
            "distance_from_high_pct": round(float(stats['distance_from_high_pct']), 1),
            "distance_from_low_pct": round(float(stats['distance_from_low_pct']), 1)
        },
        "momentum": {
            "consecutive_up_days": int(stats['consecutive_ups']),
            "consecutive_down_days": int(stats['consecutive_downs']),
            "volume_trend_pct": round(float(stats['volume_trend_pct']), 1)
        },
        "verdict": {
            "risk_level": risk_level,
            "risk_detail": risk_detail,
            "sharpe_rating": sharpe_rating,
            "sharpe_detail": sharpe_detail,
            "trend": trend,
            "trend_detail": trend_detail
        }
    }


def find_momentum_stocks(min_return: float = 5.0, min_consecutive_days: int = 3, top_n: int = 15) -> dict:
    """
    Find stocks with strong momentum (consecutive up days + positive returns).
    
    Args:
        min_return: Minimum return % over last 10 days (default 5%)
        min_consecutive_days: Minimum consecutive up days (default 3)
        top_n: Number of stocks to return (default 15)
    
    Returns:
        Dictionary with stocks showing sustained upward momentum
    """
    _ = NSESTORE.df
    
    if not NSESTORE.max_date:
        return {"tool": "find_momentum_stocks", "error": "No data available"}
    
    end_date = NSESTORE.max_date
    start_date = end_date - timedelta(days=15)  # Extra buffer
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(start_date)) & (df["DATE"] <= pd.Timestamp(end_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return {"tool": "find_momentum_stocks", "error": "No data for momentum analysis"}
    
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol, group in filtered.groupby("SYMBOL"):
        if len(group) < 5:
            continue
        
        stats = MetricsEngine.calculate_period_stats(group)
        if not stats:
            continue
        
        # Filter by criteria
        if stats['return_pct'] >= min_return and stats['consecutive_ups'] >= min_consecutive_days:
            stats['symbol'] = symbol
            results.append(stats)
    
    if not results:
        return {
            "tool": "find_momentum_stocks",
            "error": f"No momentum stocks found (return >={min_return}%, consecutive days >={min_consecutive_days})"
        }
    
    # Sort by combination of return and consecutive days
    results.sort(key=lambda x: (x['consecutive_ups'], x['return_pct']), reverse=True)
    results = results[:top_n]
    
    stocks = []
    for idx, stats in enumerate(results, 1):
        # SMA status
        sma_status = "Above SMA" if stats['end_price'] > stats['sma_20'] else "Below SMA"
        
        stocks.append({
            "rank": idx,
            "symbol": stats['symbol'],
            "return_pct": round(float(stats['return_pct']), 2),
            "consecutive_up_days": int(stats['consecutive_ups']),
            "volume_trend_pct": round(float(stats['volume_trend_pct']), 1),
            "sma_status": sma_status,
            "price_end": round(float(stats['end_price']), 2),
            "sma_20": round(float(stats['sma_20']), 2)
        })
    
    return {
        "tool": "find_momentum_stocks",
        "period": {
            "start": str(start_date),
            "end": str(end_date),
            "days": int(results[0]['days_count'])
        },
        "criteria": {
            "min_return": min_return,
            "min_consecutive_days": min_consecutive_days
        },
        "stocks": stocks,
        "summary": {
            "total_found": len(stocks),
            "strategy": "Look for volume confirmation + price above SMA for continuation"
        }
    }


def detect_reversal_candidates(lookback_days: int = 30, top_n: int = 15) -> dict:
    """
    Find oversold stocks showing early reversal signs (for contrarian plays).
    
    Args:
        lookback_days: Period for analysis (default 30 days)
        top_n: Number of candidates to return (default 15)
    
    Returns:
        Dictionary with stocks that dropped significantly but showing reversal signals
        
    Reversal signals: Large decline + recent consecutive up days + volume increase
    """
    _ = NSESTORE.df
    
    if not NSESTORE.max_date:
        return {"tool": "detect_reversal_candidates", "error": "No data available"}
    
    end_date = NSESTORE.max_date
    start_date = end_date - timedelta(days=lookback_days + 5)
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(start_date)) & (df["DATE"] <= pd.Timestamp(end_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return {"tool": "detect_reversal_candidates", "error": "No data for reversal analysis"}
    
    from investor_agent.data_engine import MetricsEngine
    
    results = []
    for symbol, group in filtered.groupby("SYMBOL"):
        if len(group) < 10:
            continue
        
        stats = MetricsEngine.calculate_period_stats(group)
        if not stats:
            continue
        
        # Reversal criteria:
        # 1. Overall negative return (oversold)
        # 2. Recent consecutive up days (reversal starting)
        # 3. Volume increasing (accumulation)
        # 4. Not at 52-week low (avoid falling knives)
        
        if (stats['return_pct'] < -5 and 
            stats['consecutive_ups'] >= 2 and 
            stats['volume_trend_pct'] > 10 and
            stats['distance_from_low_pct'] > 5):
            
            stats['symbol'] = symbol
            results.append(stats)
    
    if not results:
        return {
            "tool": "detect_reversal_candidates",
            "error": f"No reversal candidates found (last {lookback_days} days)"
        }
    
    # Sort by combination of oversold + reversal strength
    results.sort(key=lambda x: (x['consecutive_ups'], -x['return_pct']), reverse=True)
    results = results[:top_n]
    
    candidates = []
    for idx, stats in enumerate(results, 1):
        # Reversal strength
        if stats['consecutive_ups'] >= 3 and stats['volume_trend_pct'] > 30:
            signal = "Strong"
        elif stats['consecutive_ups'] >= 2 and stats['volume_trend_pct'] > 15:
            signal = "Moderate"
        else:
            signal = "Weak"
        
        candidates.append({
            "rank": idx,
            "symbol": stats['symbol'],
            "overall_return_pct": round(float(stats['return_pct']), 2),
            "consecutive_up_days": int(stats['consecutive_ups']),
            "volume_trend_pct": round(float(stats['volume_trend_pct']), 1),
            "distance_from_low_pct": round(float(stats['distance_from_low_pct']), 1),
            "signal": signal,
            "price_current": round(float(stats['end_price']), 2)
        })
    
    return {
        "tool": "detect_reversal_candidates",
        "period": {
            "lookback_days": lookback_days,
            "start": str(start_date),
            "end": str(end_date)
        },
        "criteria": {
            "min_decline": -5.0,
            "min_up_days": 2,
            "min_volume_surge": 10.0
        },
        "candidates": candidates,
        "summary": {
            "total_found": len(candidates),
            "risk_warning": "Reversal trades are counter-trend. Wait for confirmation before entry.",
            "strategy": "Look for 3+ consecutive up days + delivery % >40% for confirmation"
        }
    }


def get_volume_price_divergence(min_divergence: float = 20.0, top_n: int = 15) -> dict:
    """
    Detect volume-price divergence (price up but volume down = weak rally, and vice versa).
    
    Args:
        min_divergence: Minimum divergence % between price and volume trends (default 20%)
        top_n: Number of stocks to return (default 15)
    
    Returns:
        Dictionary with stocks showing significant volume-price divergence (warning signals)
        
    Bearish divergence: Price rising but volume declining (rally losing steam)
    Bullish divergence: Price falling but volume increasing (accumulation)
    """
    _ = NSESTORE.df
    
    if not NSESTORE.max_date:
        return {"tool": "get_volume_price_divergence", "error": "No data available"}
    
    end_date = NSESTORE.max_date
    start_date = end_date - timedelta(days=20)
    
    df = NSESTORE.df
    mask = (df["DATE"] >= pd.Timestamp(start_date)) & (df["DATE"] <= pd.Timestamp(end_date))
    filtered = df[mask].copy()
    
    if filtered.empty:
        return {"tool": "get_volume_price_divergence", "error": "No data for divergence analysis"}
    
    from investor_agent.data_engine import MetricsEngine
    
    bearish_div = []  # Price up, volume down
    bullish_div = []  # Price down, volume up
    
    for symbol, group in filtered.groupby("SYMBOL"):
        if len(group) < 10:
            continue
        
        stats = MetricsEngine.calculate_period_stats(group)
        if not stats:
            continue
        
        # Bearish: Price positive, volume negative (or vice versa with threshold)
        if stats['return_pct'] > 3 and stats['volume_trend_pct'] < -min_divergence:
            divergence_value = abs(stats['return_pct'] + stats['volume_trend_pct'])
            risk = "High" if divergence_value > 40 else "Moderate"
            bearish_div.append({
                "symbol": stats['symbol'],
                "price_return_pct": round(float(stats['return_pct']), 2),
                "volume_trend_pct": round(float(stats['volume_trend_pct']), 2),
                "divergence": round(float(divergence_value), 1),
                "risk": risk
            })
        
        # Bullish: Price negative, volume positive
        if stats['return_pct'] < -3 and stats['volume_trend_pct'] > min_divergence:
            divergence_value = abs(stats['return_pct'] - stats['volume_trend_pct'])
            opportunity = "High" if divergence_value > 40 else "Moderate"
            bullish_div.append({
                "symbol": stats['symbol'],
                "price_return_pct": round(float(stats['return_pct']), 2),
                "volume_trend_pct": round(float(stats['volume_trend_pct']), 2),
                "divergence": round(float(divergence_value), 1),
                "opportunity": opportunity
            })
    
    # Sort by divergence strength
    bearish_div.sort(key=lambda x: x['divergence'], reverse=True)
    bullish_div.sort(key=lambda x: x['divergence'], reverse=True)
    
    return {
        "tool": "get_volume_price_divergence",
        "period": {
            "start": str(start_date),
            "end": str(end_date),
            "days": 20
        },
        "min_divergence_threshold": min_divergence,
        "bearish_divergence": {
            "description": "Price rising but volume declining - rally losing steam, potential reversal",
            "stocks": bearish_div[:top_n]
        },
        "bullish_divergence": {
            "description": "Price falling but volume increasing - accumulation during decline, potential reversal",
            "stocks": bullish_div[:top_n]
        },
        "summary": {
            "bearish_count": len(bearish_div),
            "bullish_count": len(bullish_div),
            "interpretation": "Divergences indicate potential trend reversals - confirm with delivery % and price action"
        }
    }


# ==============================================================================
# NEWS SEARCH TOOLS - SEMANTIC SEARCH OVER INGESTED PDFs
# ==============================================================================

# Cache for symbol-to-name mapping
_SYMBOL_NAME_MAP = None

def get_company_name(symbol: str) -> dict:
    """
    Convert stock symbol to company name using NSE symbol-company mapping.
    
    Args:
        symbol: Stock ticker (e.g., 'RELIANCE', 'TCS', 'SVPGLOB')
    
    Returns:
        Dictionary with symbol, company_name, and whether mapping was found
        
    Example:
        >>> get_company_name('RELIANCE')
        {{'symbol': 'RELIANCE', 'company_name': 'Reliance Industries Limited', 'found': True}}
        
        >>> get_company_name('UNKNOWN')
        {{'symbol': 'UNKNOWN', 'company_name': 'UNKNOWN', 'found': False}}
    """
    global _SYMBOL_NAME_MAP
    
    # Load mapping on first call (lazy initialization)
    if _SYMBOL_NAME_MAP is None:
        csv_path = os.path.join(
            os.path.dirname(__file__), 
            "data", 
            "nse_symbol_company_mapping.csv"
        )
        
        if not os.path.exists(csv_path):
            logger.warning(f"NSE symbol-company mapping not found at {csv_path}")
            return {
                "symbol": symbol,
                "company_name": symbol,  # Fallback to symbol
                "found": False,
                "error": "nse_symbol_company_mapping.csv not found"
            }
        
        try:
            # Read CSV and create symbol->name mapping
            df = pd.read_csv(csv_path)
            # Strip whitespace from column names and values
            df.columns = df.columns.str.strip()
            _SYMBOL_NAME_MAP = dict(zip(
                df['SYMBOL'].str.strip().str.upper(),
                df['NAME OF COMPANY'].str.strip()
            ))
            logger.info(f"Loaded {len(_SYMBOL_NAME_MAP)} symbol-to-name mappings from NSE")
        except Exception as e:
            logger.error(f"Failed to load NSE symbol-company mapping: {e}")
            return {
                "symbol": symbol,
                "company_name": symbol,
                "found": False,
                "error": str(e)
            }
    
    # Lookup symbol (case-insensitive)
    symbol_upper = symbol.strip().upper()
    company_name = _SYMBOL_NAME_MAP.get(symbol_upper)
    
    if company_name:
        return {
            "symbol": symbol,
            "company_name": company_name,
            "found": True
        }
    else:
        logger.warning(f"Symbol '{symbol}' not found in NSE mapping")
        return {
            "symbol": symbol,
            "company_name": symbol,  # Fallback to symbol itself
            "found": False
        }


def get_monthly_dirs_for_date_range(
    start_date: str,
    end_date: str,
    base_dir: str = "./investor_agent/data/vector-data",
) -> list[str]:
    """Generate list of monthly directory paths for a date range.
    
    Only returns directories that actually exist on disk.
    
    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
        base_dir: Base directory containing monthly subdirectories
        
    Returns:
        List of existing directory paths (e.g., ['./vector-data/202407', './vector-data/202408'])
        
    Example:
        >>> get_monthly_dirs_for_date_range('2024-07-15', '2024-09-20')
        ['./investor_agent/data/vector-data/202407', './investor_agent/data/vector-data/202408', './investor_agent/data/vector-data/202409']
    """
    from datetime import datetime
    from dateutil.relativedelta import relativedelta
    from pathlib import Path
    
    try:
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError:
        logger.warning("Invalid date format (%s to %s), using all available months", start_date, end_date)
        return []
    
    # Generate YYYYMM for each month in range
    monthly_dirs = []
    current = start.replace(day=1)  # Start from beginning of month
    end_month = end.replace(day=1)
    
    while current <= end_month:
        month_str = current.strftime("%Y%m")
        dir_path = f"{base_dir}/{month_str}"
        
        # Only include if directory exists
        if Path(dir_path).exists() and Path(dir_path).is_dir():
            monthly_dirs.append(dir_path)
        else:
            logger.debug("Skipping non-existent directory: %s", dir_path)
        
        current += relativedelta(months=1)
    
    if not monthly_dirs:
        logger.warning(
            "No existing directories found for date range %s to %s in %s",
            start_date, end_date, base_dir
        )
    
    return monthly_dirs


def init_search_resources(
    persist_dir: str | None = None,
    collection_name: str = "pdf_chunks",
    model_name: str = "intfloat/multilingual-e5-base",
) -> None:
    """Initialize ChromaDB collections and embedding model for semantic search.
    
    Supports loading multiple monthly collections (e.g., 202407, 202408, 202409).
    
    Args:
        persist_dir: Comma or colon-separated list of directories containing ChromaDB data.
                    Can be single directory or multiple (e.g., "./investor_agent/data/vector-data/202407,./investor_agent/data/vector-data/202408").
                    If None, uses NEWS_PERSIST_DIR environment variable or defaults to './investor_agent/data/vector-data'
        collection_name: Name of the ChromaDB collection to load (default: 'pdf_chunks')
        model_name: SentenceTransformer model name for embeddings (default: 'intfloat/multilingual-e5-base')
    """
    if not _SEMANTIC_SEARCH_AVAILABLE:
        logger.error("Cannot initialize search resources - dependencies not installed")
        return
    
    # Allow reinitialization if persist_dir is explicitly provided (dynamic loading)
    if _search_state.initialized and persist_dir is None:
        logger.debug("Search resources already initialized, skipping")
        return
    
    if persist_dir is None:
        persist_dir = os.environ.get("NEWS_PERSIST_DIR", "./investor_agent/data/vector-data")
    
    # Split on comma and os.pathsep (':'), strip empties
    raw_parts = []
    for segment in persist_dir.split(","):
        raw_parts.extend(segment.split(os.pathsep))
    dirs = [p.strip() for p in raw_parts if p.strip()]
    if not dirs:
        dirs = ["./vector-data"]
    
    logger.info("Loading ChromaDB collections from %d director%s: %s", 
                len(dirs), "y" if len(dirs) == 1 else "ies", ", ".join(dirs))
    
    # Load collections from all directories
    collections = []
    for d in dirs:
        try:
            persistent_client = chromadb.PersistentClient(path=d)
            collection = persistent_client.get_collection(collection_name)
            collections.append(collection)
            logger.info(
                "âœ“ Loaded collection '%s' from '%s' (count=%d)",
                collection_name,
                d,
                collection.count(),
            )
        except Exception as e:  # noqa: BLE001
            logger.warning("âœ— Skipping '%s' due to load error: %s", d, e)
    
    if not collections:
        logger.error("âŒ No collections loaded from provided directories: %s", dirs)
        return
    
    _search_state.collections = collections
    _search_state.model = SentenceTransformer(model_name)
    _search_state.initialized = True
    logger.info("âœ… News search resources initialized (model=%s, collections=%d)", model_name, len(collections))


def semantic_search(
    query: str,
    n_results: int = 5,
    min_similarity: float = 0.3,
) -> list[dict]:
    """Performs semantic search on ingested news PDFs and returns matching documents.

    This function searches through locally stored PDF chunks (Economic Times, etc.)
    using sentence embeddings. It's designed to be used as a tool by the NewsAnalyst
    agent to find relevant news for stock analysis.

    Args:
        query: The search query string (e.g., "RELIANCE earnings November 2025").
        n_results: Number of results to return. Defaults to 5.
        min_similarity: Minimum similarity threshold (0-1).
            Defaults to 0.3. Lower threshold = more results but less relevant.

    Returns:
        list[dict]: List of dictionaries, each containing:
            - 'document' (str): The document text content (PDF chunk)
            - 'metadata' (dict): Metadata including source file and chunk index
            - 'similarity' (float): Similarity score (0-1, higher is better)

    Example:
        >>> results = semantic_search("TCS quarterly results", n_results=3)
        >>> for result in results:
        ...     print(f"Similarity: {result['similarity']}")
        ...     print(f"Source: {result['metadata']['source']}")
        ...     print(f"Content: {result['document'][:200]}...")
    """
    if not _SEMANTIC_SEARCH_AVAILABLE:
        logger.error("semantic_search called but dependencies not installed")
        return []
    
    # Ensure resources are initialized
    if not _search_state.initialized:
        init_search_resources()
    
    if not _search_state.collections or _search_state.model is None:
        logger.error("semantic_search called but resources not initialized")
        return []

    # Add query prefix required by multilingual-e5-base model
    prefixed_query = f"query: {query}"
    query_embedding = _search_state.model.encode(prefixed_query)
    # Convert to plain list[float] if needed for Chroma types
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()
    qe_list = cast(list[float], query_embedding)

    # Perform semantic search across all collections
    aggregate_results = []
    for col in _search_state.collections:
        results = col.query(query_embeddings=[qe_list], n_results=n_results)
        if not results or not results.get("documents"):
            continue
        documents = results["documents"][0]  # type: ignore[index]
        metadatas = results["metadatas"][0]  # type: ignore[index]
        scores_or_distances = results.get("distances",
                  results.get("scores", [[]]))[0]  # type: ignore[index]
        for doc, meta, score in zip(
            documents,
            metadatas,
            scores_or_distances,
        ):
            if "scores" in results:
                similarity = score
            elif "distances" in results:
                similarity = 1 - score
            else:
                similarity = None
            if similarity is not None and similarity >= min_similarity:
                aggregate_results.append(
                    {
                        "document": doc,
                        "metadata": meta,
                        "similarity": round(similarity, 4),
                    }
                )
    # Sort combined results by similarity desc and truncate
    aggregate_results.sort(key=lambda r: r["similarity"], reverse=True)
    return aggregate_results[:n_results]


def load_collections_for_date_range(
    start_date: str,
    end_date: str,
    base_dir: str = "./investor_agent/data/vector-data",
    collection_name: str = "pdf_chunks",
) -> bool:
    """Dynamically load collections for specific date range.
    
    This function determines which monthly directories to load based on the
    query date range and reinitializes the search resources.
    
    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        base_dir: Base directory containing monthly subdirectories (default: ./investor_agent/data/vector-data)
                 Can be overridden via NEWS_BASE_DIR environment variable
        collection_name: ChromaDB collection name
        
    Returns:
        True if collections loaded successfully, False otherwise
        
    Example:
        >>> load_collections_for_date_range('2024-07-01', '2024-09-30')
        # Loads ./investor_agent/data/vector-data/202407, ./investor_agent/data/vector-data/202408, ./investor_agent/data/vector-data/202409
    """
    if not _SEMANTIC_SEARCH_AVAILABLE:
        logger.error("Semantic search dependencies not available")
        return False
    
    # Allow override via environment variable
    base_dir = os.environ.get("NEWS_BASE_DIR", base_dir)
    
    # Get monthly directories for date range (only existing ones)
    monthly_dirs = get_monthly_dirs_for_date_range(start_date, end_date, base_dir)
    
    if not monthly_dirs:
        logger.error("No existing directories found for date range %s to %s in %s", 
                    start_date, end_date, base_dir)
        return False
    
    logger.info("ðŸ“… Loading collections for date range %s to %s", start_date, end_date)
    logger.info("   Directories: %s", ", ".join([os.path.basename(d) for d in monthly_dirs]))
    
    # Clear existing state to force reinitialization
    _search_state.initialized = False
    _search_state.collections = []
    _search_state.model = None
    
    # Load collections from the determined directories
    collections = []
    for dir_path in monthly_dirs:
        try:
            persistent_client = chromadb.PersistentClient(path=dir_path)
            collection = persistent_client.get_collection(collection_name)
            collections.append(collection)
            logger.info(
                "   âœ“ Loaded '%s' from %s (count=%d)",
                collection_name,
                os.path.basename(dir_path),
                collection.count(),
            )
        except Exception as e:
            logger.warning("   âœ— Failed to load from %s: %s", dir_path, e)
    
    if not collections:
        logger.error("âŒ No collections loaded successfully")
        return False
    
    # Initialize the model (only once)
    if _search_state.model is None:
        try:
            _search_state.model = SentenceTransformer("intfloat/multilingual-e5-base")
            logger.info("   âœ“ Loaded embedding model: intfloat/multilingual-e5-base")
        except Exception as e:
            logger.error("   âœ— Failed to load embedding model: %s", e)
            return False
    
    # Update state
    _search_state.collections = collections
    _search_state.initialized = True
    
    logger.info("âœ… Successfully loaded %d collection(s)", len(collections))
    return True